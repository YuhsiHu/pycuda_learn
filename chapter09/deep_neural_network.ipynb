{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Starting training epoch: 0\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.60894396584712\n",
      "entropy: 0.6037831012887515\n",
      "entropy: 0.6159294359591069\n",
      "entropy: 0.6099731150903818\n",
      "entropy: 0.6130514453594663\n",
      "entropy: 0.6072207077581612\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 1\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.6040925823660331\n",
      "entropy: 0.5995956460175048\n",
      "entropy: 0.6165776198323216\n",
      "entropy: 0.6070644133182027\n",
      "entropy: 0.6094802458809593\n",
      "entropy: 0.601932936400771\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 2\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5918687856277275\n",
      "entropy: 0.5881038743151723\n",
      "entropy: 0.6166531234632799\n",
      "entropy: 0.5991332993511088\n",
      "entropy: 0.6000920631288091\n",
      "entropy: 0.5870626135605347\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 3\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5766471979813623\n",
      "entropy: 0.5748021002903532\n",
      "entropy: 0.6167890986455452\n",
      "entropy: 0.5896005747169488\n",
      "entropy: 0.5856991118355328\n",
      "entropy: 0.5666845284512113\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 4\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5619895542501222\n",
      "entropy: 0.5633611629552856\n",
      "entropy: 0.6164110223312647\n",
      "entropy: 0.5814238686631309\n",
      "entropy: 0.5704246295266125\n",
      "entropy: 0.5479181055190823\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 5\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5490312502672916\n",
      "entropy: 0.5529998175261895\n",
      "entropy: 0.6191098561651224\n",
      "entropy: 0.5731445416643294\n",
      "entropy: 0.5599308463745728\n",
      "entropy: 0.5280603607738209\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 6\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5360614566752955\n",
      "entropy: 0.5443837150625056\n",
      "entropy: 0.614892360553812\n",
      "entropy: 0.5666997859605405\n",
      "entropy: 0.5436410835057325\n",
      "entropy: 0.5145339814391546\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 7\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5251433289545009\n",
      "entropy: 0.5355818177371195\n",
      "entropy: 0.6197611725073838\n",
      "entropy: 0.5607734970173132\n",
      "entropy: 0.5343006099856561\n",
      "entropy: 0.49417717339271255\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 8\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5130601525193691\n",
      "entropy: 0.5247794817595671\n",
      "entropy: 0.6105175783584211\n",
      "entropy: 0.5494715587439326\n",
      "entropy: 0.5164953947926985\n",
      "entropy: 0.4808897326085147\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 9\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.5004810790350714\n",
      "entropy: 0.5065798741091968\n",
      "entropy: 0.6104095686847949\n",
      "entropy: 0.5379374648245123\n",
      "entropy: 0.5055106116086261\n",
      "entropy: 0.4578965351114937\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 10\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.4861099471784442\n",
      "entropy: 0.49079620867994384\n",
      "entropy: 0.6045692115079846\n",
      "entropy: 0.5234880897163802\n",
      "entropy: 0.489669967487518\n",
      "entropy: 0.43994034596925574\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 11\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.47052614097721235\n",
      "entropy: 0.4739058568498198\n",
      "entropy: 0.5969940719566412\n",
      "entropy: 0.511273193156462\n",
      "entropy: 0.47241845234681407\n",
      "entropy: 0.42084630348887425\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 12\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.44079296270782015\n",
      "entropy: 0.4602716810625616\n",
      "entropy: 0.5631428716245083\n",
      "entropy: 0.5031258599570464\n",
      "entropy: 0.4132304067278425\n",
      "entropy: 0.41188011753161896\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 13\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3996697337001973\n",
      "entropy: 0.44784861849335106\n",
      "entropy: 0.5045108601907944\n",
      "entropy: 0.5006412424781249\n",
      "entropy: 0.3444070464526668\n",
      "entropy: 0.4070289930668544\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 14\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3728970205878898\n",
      "entropy: 0.4248467708267949\n",
      "entropy: 0.49271804645304584\n",
      "entropy: 0.4775246793280043\n",
      "entropy: 0.3192800625160018\n",
      "entropy: 0.38156585264543263\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 15\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35636064429781583\n",
      "entropy: 0.4170843369244304\n",
      "entropy: 0.46881785374560164\n",
      "entropy: 0.47135283176137555\n",
      "entropy: 0.2941857846830546\n",
      "entropy: 0.3718237594445742\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 16\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3421921611270469\n",
      "entropy: 0.40675616632022865\n",
      "entropy: 0.4570225523323333\n",
      "entropy: 0.46265034729646315\n",
      "entropy: 0.27885195188815604\n",
      "entropy: 0.3599184666716541\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 17\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3306724164919836\n",
      "entropy: 0.3941188826647212\n",
      "entropy: 0.45386575881201235\n",
      "entropy: 0.4481476163998254\n",
      "entropy: 0.27053951033203777\n",
      "entropy: 0.3465975808649981\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 18\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3204345896581571\n",
      "entropy: 0.381838747181873\n",
      "entropy: 0.47650087322864715\n",
      "entropy: 0.4341830372753001\n",
      "entropy: 0.2785084380249787\n",
      "entropy: 0.32149106729280646\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 19\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3248644139427609\n",
      "entropy: 0.38148951424944294\n",
      "entropy: 0.4215108758442347\n",
      "entropy: 0.4586294197659053\n",
      "entropy: 0.2562462775660139\n",
      "entropy: 0.3302192715026057\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 20\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3272368127975182\n",
      "entropy: 0.37272540649996805\n",
      "entropy: 0.4962595847666213\n",
      "entropy: 0.41650046087498765\n",
      "entropy: 0.30029520499298806\n",
      "entropy: 0.2999538694601131\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 21\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3092339089370937\n",
      "entropy: 0.3740366980332536\n",
      "entropy: 0.4208921417604236\n",
      "entropy: 0.44236085965620103\n",
      "entropy: 0.2484570521201149\n",
      "entropy: 0.31824868422028924\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 22\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.2977737301496029\n",
      "entropy: 0.3692199957554923\n",
      "entropy: 0.4613082642326492\n",
      "entropy: 0.4131052540724822\n",
      "entropy: 0.26015857282759086\n",
      "entropy: 0.29880939030527187\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 23\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.33798173647870344\n",
      "entropy: 0.34679735241600773\n",
      "entropy: 0.38632007729548534\n",
      "entropy: 0.4517299330365008\n",
      "entropy: 0.22479836852589274\n",
      "entropy: 0.31919279477730517\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 24\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.33508489556939053\n",
      "entropy: 0.36819251488580146\n",
      "entropy: 0.5622499491824043\n",
      "entropy: 0.4169848856421632\n",
      "entropy: 0.37777097687328937\n",
      "entropy: 0.286785570253598\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 25\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.28121949669478996\n",
      "entropy: 0.36838292194895234\n",
      "entropy: 0.4248812432611502\n",
      "entropy: 0.3988688485531036\n",
      "entropy: 0.24129948891916372\n",
      "entropy: 0.3028120999888454\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 26\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.32168672400712855\n",
      "entropy: 0.3646329421804106\n",
      "entropy: 0.5232643821426759\n",
      "entropy: 0.40232814756208085\n",
      "entropy: 0.33987642250517003\n",
      "entropy: 0.28210416835279944\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 27\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.31825020014381344\n",
      "entropy: 0.36398199778879875\n",
      "entropy: 0.475957653466821\n",
      "entropy: 0.39977951357554126\n",
      "entropy: 0.28621940809662605\n",
      "entropy: 0.28219524413096625\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 28\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3138513402602971\n",
      "entropy: 0.33840683870499194\n",
      "entropy: 0.37831493618556905\n",
      "entropy: 0.4445269745774159\n",
      "entropy: 0.22318177154042287\n",
      "entropy: 0.30125993604102025\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 29\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.29757943639458434\n",
      "entropy: 0.3634277421930608\n",
      "entropy: 0.45415390297369446\n",
      "entropy: 0.39936050801566475\n",
      "entropy: 0.25512177594161506\n",
      "entropy: 0.2819122076118917\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 30\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35787653105908596\n",
      "entropy: 0.3121622611905271\n",
      "entropy: 0.34049580786448\n",
      "entropy: 0.4014699047154839\n",
      "entropy: 0.22713869637035797\n",
      "entropy: 0.314313120621797\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 31\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35738866603192165\n",
      "entropy: 0.3127985366700918\n",
      "entropy: 0.34215705759046355\n",
      "entropy: 0.4033979519597135\n",
      "entropy: 0.22391605374966664\n",
      "entropy: 0.31497146294762846\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 32\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35696788019956327\n",
      "entropy: 0.3125131583372703\n",
      "entropy: 0.34373852669221866\n",
      "entropy: 0.4053272201942959\n",
      "entropy: 0.22123818789494396\n",
      "entropy: 0.3157304712522169\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 33\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3566101970037448\n",
      "entropy: 0.3109915334506365\n",
      "entropy: 0.34500547304555806\n",
      "entropy: 0.406993273789507\n",
      "entropy: 0.2191661354413397\n",
      "entropy: 0.3163828075865191\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 34\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3562910554975424\n",
      "entropy: 0.3094409055208157\n",
      "entropy: 0.3462225750828562\n",
      "entropy: 0.40865237867818927\n",
      "entropy: 0.21734388208527364\n",
      "entropy: 0.31708142060512284\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 35\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35600453583063185\n",
      "entropy: 0.3081545738686695\n",
      "entropy: 0.34735559160473684\n",
      "entropy: 0.4102484716831288\n",
      "entropy: 0.21576652954476108\n",
      "entropy: 0.3177763847629745\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 36\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35574819129391816\n",
      "entropy: 0.3069860354672014\n",
      "entropy: 0.34837022314742744\n",
      "entropy: 0.411804577465779\n",
      "entropy: 0.2144105252518443\n",
      "entropy: 0.31843438775019106\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 37\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3555144731133362\n",
      "entropy: 0.3059954270926356\n",
      "entropy: 0.34933860054204297\n",
      "entropy: 0.4129586418582207\n",
      "entropy: 0.2131993064859232\n",
      "entropy: 0.31909618621136104\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 38\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3553006313335088\n",
      "entropy: 0.3050705158722432\n",
      "entropy: 0.3502366495700915\n",
      "entropy: 0.41361943007850277\n",
      "entropy: 0.2121190119949713\n",
      "entropy: 0.319738694903236\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 39\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3551055277669266\n",
      "entropy: 0.30427159484021143\n",
      "entropy: 0.35107937270967793\n",
      "entropy: 0.41349626985658466\n",
      "entropy: 0.21115562869506443\n",
      "entropy: 0.3203640814942536\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 40\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35492590207032243\n",
      "entropy: 0.3035014286514451\n",
      "entropy: 0.35171128947489044\n",
      "entropy: 0.41336641270648494\n",
      "entropy: 0.2102953478487988\n",
      "entropy: 0.3209559393826879\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 41\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35476124279460636\n",
      "entropy: 0.3028056529129278\n",
      "entropy: 0.352101711318298\n",
      "entropy: 0.4132702632221523\n",
      "entropy: 0.20950809856234373\n",
      "entropy: 0.32154663089543306\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 42\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35460885252562563\n",
      "entropy: 0.3022067772574735\n",
      "entropy: 0.35248051273400877\n",
      "entropy: 0.41319876122761506\n",
      "entropy: 0.20878917476194073\n",
      "entropy: 0.3221305274693283\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 43\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35446864643437676\n",
      "entropy: 0.30162574672293957\n",
      "entropy: 0.3527446176780808\n",
      "entropy: 0.4131394678710832\n",
      "entropy: 0.20814269872544247\n",
      "entropy: 0.3225123467728408\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 44\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3543375710948206\n",
      "entropy: 0.3011130768630536\n",
      "entropy: 0.35270815855325344\n",
      "entropy: 0.413096188644202\n",
      "entropy: 0.20753548555460208\n",
      "entropy: 0.32270265013338434\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 45\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35421692820168077\n",
      "entropy: 0.30062446006719395\n",
      "entropy: 0.35267855253074804\n",
      "entropy: 0.41306355660449223\n",
      "entropy: 0.20697841417928306\n",
      "entropy: 0.3228982775344084\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 46\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35410402153368375\n",
      "entropy: 0.300161634208775\n",
      "entropy: 0.35265912180472414\n",
      "entropy: 0.4130410139344467\n",
      "entropy: 0.20646446773547714\n",
      "entropy: 0.3228021671887981\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 47\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.353999004623953\n",
      "entropy: 0.2997409972414326\n",
      "entropy: 0.3526455393219514\n",
      "entropy: 0.4130249385537592\n",
      "entropy: 0.2059971831577566\n",
      "entropy: 0.3226020505245301\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 48\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35389972674351905\n",
      "entropy: 0.29930253554079456\n",
      "entropy: 0.352638955891585\n",
      "entropy: 0.4130146224825593\n",
      "entropy: 0.20556997010185638\n",
      "entropy: 0.3224216826070333\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 49\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3538073959653228\n",
      "entropy: 0.2989003411146132\n",
      "entropy: 0.3526527211846049\n",
      "entropy: 0.4130103639560754\n",
      "entropy: 0.20517338542070496\n",
      "entropy: 0.3222606615754375\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 50\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35371999773025004\n",
      "entropy: 0.2986175150224374\n",
      "entropy: 0.3526673963721596\n",
      "entropy: 0.4130580757337788\n",
      "entropy: 0.20473834220107434\n",
      "entropy: 0.32209971561052486\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 51\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3536384152456395\n",
      "entropy: 0.29836297726314304\n",
      "entropy: 0.35268691516882456\n",
      "entropy: 0.4131195431904775\n",
      "entropy: 0.204376591753168\n",
      "entropy: 0.32196386382979386\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 52\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35356178421122086\n",
      "entropy: 0.2981693078328262\n",
      "entropy: 0.3527082833530571\n",
      "entropy: 0.41319073933118294\n",
      "entropy: 0.20400718846999769\n",
      "entropy: 0.32183200154603975\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 53\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3534896443123318\n",
      "entropy: 0.2978680509666859\n",
      "entropy: 0.35273458143442227\n",
      "entropy: 0.4132560560656615\n",
      "entropy: 0.20372211498150647\n",
      "entropy: 0.32173143848834546\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 54\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35342100025324336\n",
      "entropy: 0.297747147315558\n",
      "entropy: 0.35276399415200954\n",
      "entropy: 0.41334615243460243\n",
      "entropy: 0.20336780899637416\n",
      "entropy: 0.32161817626320116\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 55\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35335805473901777\n",
      "entropy: 0.2976400519997591\n",
      "entropy: 0.3527955894194843\n",
      "entropy: 0.4134332054626457\n",
      "entropy: 0.2030467984575386\n",
      "entropy: 0.3215167285931437\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 56\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3533173990344796\n",
      "entropy: 0.2975996521381979\n",
      "entropy: 0.3528324858464618\n",
      "entropy: 0.4135361424189374\n",
      "entropy: 0.20270563579206188\n",
      "entropy: 0.32142039387533666\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 57\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35328028734886496\n",
      "entropy: 0.29674364079638527\n",
      "entropy: 0.3518367888882034\n",
      "entropy: 0.4120520655517719\n",
      "entropy: 0.20272359927883765\n",
      "entropy: 0.3205221270318802\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 58\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35322313496387875\n",
      "entropy: 0.2966858975186948\n",
      "entropy: 0.35185797192898577\n",
      "entropy: 0.4120917351798153\n",
      "entropy: 0.20238727253143593\n",
      "entropy: 0.320413238479935\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 59\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3531689763627968\n",
      "entropy: 0.29673897400467963\n",
      "entropy: 0.35188042590503393\n",
      "entropy: 0.41214540790407006\n",
      "entropy: 0.20201691891214582\n",
      "entropy: 0.3202960796665763\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 60\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35311733414907676\n",
      "entropy: 0.2964251331362226\n",
      "entropy: 0.35190566202754736\n",
      "entropy: 0.4121826314305839\n",
      "entropy: 0.20173970648947726\n",
      "entropy: 0.32020941238573336\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 61\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35306899896024885\n",
      "entropy: 0.2961909231186279\n",
      "entropy: 0.3519329894681333\n",
      "entropy: 0.41223749828751527\n",
      "entropy: 0.20142269478795735\n",
      "entropy: 0.3201134260946407\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 62\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3530216698233066\n",
      "entropy: 0.29579688875068716\n",
      "entropy: 0.35195743695019505\n",
      "entropy: 0.4122480702056141\n",
      "entropy: 0.20126618262237272\n",
      "entropy: 0.320061720926144\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 63\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35297768688713466\n",
      "entropy: 0.2953085518758822\n",
      "entropy: 0.35198754800884086\n",
      "entropy: 0.41230356288114317\n",
      "entropy: 0.20098730711756255\n",
      "entropy: 0.31998080353266295\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 64\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3529353275130886\n",
      "entropy: 0.29486778722068535\n",
      "entropy: 0.352017936661219\n",
      "entropy: 0.41234257933162244\n",
      "entropy: 0.20077696828126376\n",
      "entropy: 0.3199228858012538\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 65\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35289499449041023\n",
      "entropy: 0.2944348670228451\n",
      "entropy: 0.3520516952285952\n",
      "entropy: 0.4124042792793754\n",
      "entropy: 0.20052103093807228\n",
      "entropy: 0.3198532480762744\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 66\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.352856523140561\n",
      "entropy: 0.2940352853208799\n",
      "entropy: 0.35208461907341615\n",
      "entropy: 0.4124493536479948\n",
      "entropy: 0.20032847132853904\n",
      "entropy: 0.3198047087141432\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 67\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35281922675748856\n",
      "entropy: 0.2936443154409714\n",
      "entropy: 0.35212053184371195\n",
      "entropy: 0.41252630131600415\n",
      "entropy: 0.20009872620728572\n",
      "entropy: 0.3197470500338219\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 68\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35278385145019836\n",
      "entropy: 0.29331356083240817\n",
      "entropy: 0.3521547139861146\n",
      "entropy: 0.4126060551025432\n",
      "entropy: 0.19992037632296003\n",
      "entropy: 0.31970554614082397\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 69\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35274962398311455\n",
      "entropy: 0.29300418718525895\n",
      "entropy: 0.3521923794719894\n",
      "entropy: 0.4127022173982797\n",
      "entropy: 0.1997151022081597\n",
      "entropy: 0.31965774282809123\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 70\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3527168111677068\n",
      "entropy: 0.29269789323545653\n",
      "entropy: 0.35222797576098\n",
      "entropy: 0.4127798222398306\n",
      "entropy: 0.1995643481941254\n",
      "entropy: 0.3196282212232277\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 71\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.352685364778214\n",
      "entropy: 0.2924157511753955\n",
      "entropy: 0.3522680074147335\n",
      "entropy: 0.41288181538164787\n",
      "entropy: 0.199371170425241\n",
      "entropy: 0.3195877553014143\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 72\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3526552564146588\n",
      "entropy: 0.2921277115534501\n",
      "entropy: 0.35230425818532113\n",
      "entropy: 0.4129579660220685\n",
      "entropy: 0.1992364139579763\n",
      "entropy: 0.319565330837997\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 73\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3526263134361083\n",
      "entropy: 0.2918771044816318\n",
      "entropy: 0.3523428155801886\n",
      "entropy: 0.4130484978637588\n",
      "entropy: 0.19907285581938503\n",
      "entropy: 0.319534537429336\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 74\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3525985978943444\n",
      "entropy: 0.29163892776947276\n",
      "entropy: 0.35238497387729745\n",
      "entropy: 0.41315183779532627\n",
      "entropy: 0.19890254068231095\n",
      "entropy: 0.3195045149531566\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 75\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3525714059654785\n",
      "entropy: 0.2913856354110506\n",
      "entropy: 0.35242519107359666\n",
      "entropy: 0.4132361732410337\n",
      "entropy: 0.19877552547461066\n",
      "entropy: 0.3194893874558062\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 76\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35258208930756335\n",
      "entropy: 0.29118047175575473\n",
      "entropy: 0.352468341680441\n",
      "entropy: 0.41333820069303573\n",
      "entropy: 0.19861875589330746\n",
      "entropy: 0.3194650143978745\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 77\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35256085797127684\n",
      "entropy: 0.29054476439784144\n",
      "entropy: 0.35082208914790325\n",
      "entropy: 0.41088063533605484\n",
      "entropy: 0.1988775399158826\n",
      "entropy: 0.31832653600242755\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 78\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3525356237738338\n",
      "entropy: 0.2902813320013527\n",
      "entropy: 0.3508337669988883\n",
      "entropy: 0.4109131327860052\n",
      "entropy: 0.19871473951692542\n",
      "entropy: 0.31824349756633413\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 79\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35251111269611496\n",
      "entropy: 0.29000967909819053\n",
      "entropy: 0.3508415094623006\n",
      "entropy: 0.41093119848522325\n",
      "entropy: 0.19860198703221466\n",
      "entropy: 0.31817603283608037\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 80\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35248770110973454\n",
      "entropy: 0.2897819512973942\n",
      "entropy: 0.3508538371430536\n",
      "entropy: 0.4109742226897654\n",
      "entropy: 0.1984518156812489\n",
      "entropy: 0.31809856992819957\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 81\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35246515176750054\n",
      "entropy: 0.2895659142128234\n",
      "entropy: 0.3508645215867284\n",
      "entropy: 0.41100403215559705\n",
      "entropy: 0.19832492087721362\n",
      "entropy: 0.31802835250101374\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 82\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35244324795769727\n",
      "entropy: 0.28932102063445314\n",
      "entropy: 0.3508747232018181\n",
      "entropy: 0.41103048482939547\n",
      "entropy: 0.1982185156741022\n",
      "entropy: 0.3179667820854358\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 83\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3524220071574393\n",
      "entropy: 0.2891179792554275\n",
      "entropy: 0.35088850204806393\n",
      "entropy: 0.41107472298420017\n",
      "entropy: 0.19808469098984025\n",
      "entropy: 0.3178973057954741\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 84\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3524018486231169\n",
      "entropy: 0.2889017912023992\n",
      "entropy: 0.35089970222130623\n",
      "entropy: 0.41110431553437315\n",
      "entropy: 0.1979794385047923\n",
      "entropy: 0.3178376133547823\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 85\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35238202998989987\n",
      "entropy: 0.28870528987806227\n",
      "entropy: 0.3509132057182251\n",
      "entropy: 0.41114621597566053\n",
      "entropy: 0.19786169768692016\n",
      "entropy: 0.3177746293110046\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 86\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3523628885037428\n",
      "entropy: 0.2884797925407902\n",
      "entropy: 0.3509232417721245\n",
      "entropy: 0.4111669406970652\n",
      "entropy: 0.19777855482714993\n",
      "entropy: 0.3177236596121839\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 87\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3523445833833902\n",
      "entropy: 0.2883091499536122\n",
      "entropy: 0.3509384008059553\n",
      "entropy: 0.4112112239602519\n",
      "entropy: 0.1976627953280856\n",
      "entropy: 0.31766386308579925\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 88\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35232688924785815\n",
      "entropy: 0.2881521041864354\n",
      "entropy: 0.3509525942361405\n",
      "entropy: 0.411247403219051\n",
      "entropy: 0.19755968294034018\n",
      "entropy: 0.31760756841500165\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 89\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3523088983523797\n",
      "entropy: 0.28792973441355324\n",
      "entropy: 0.35096441846816573\n",
      "entropy: 0.4112729140321672\n",
      "entropy: 0.1974819654697223\n",
      "entropy: 0.31758269229175023\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 90\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3522922566230833\n",
      "entropy: 0.28777534394994336\n",
      "entropy: 0.3509801070125223\n",
      "entropy: 0.41131740955768453\n",
      "entropy: 0.19737727579516473\n",
      "entropy: 0.3175570630911329\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 91\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3522757048285499\n",
      "entropy: 0.2875790119369766\n",
      "entropy: 0.35099348477762166\n",
      "entropy: 0.41134811443785096\n",
      "entropy: 0.1972998412930789\n",
      "entropy: 0.3175448607636115\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 92\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3522598291126748\n",
      "entropy: 0.2874296502041056\n",
      "entropy: 0.3510097270190613\n",
      "entropy: 0.4113918031611352\n",
      "entropy: 0.19720384237752417\n",
      "entropy: 0.3175266587688385\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 93\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35224472341712604\n",
      "entropy: 0.28729127727595866\n",
      "entropy: 0.35102578989941874\n",
      "entropy: 0.41143490583549025\n",
      "entropy: 0.19710997426559876\n",
      "entropy: 0.31750921932688253\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 94\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3522300780788119\n",
      "entropy: 0.2871626393800079\n",
      "entropy: 0.35104308379183174\n",
      "entropy: 0.4114808617862618\n",
      "entropy: 0.19701528071648888\n",
      "entropy: 0.3174935802772579\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 95\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3522159296756267\n",
      "entropy: 0.28708843978417314\n",
      "entropy: 0.3510634514348642\n",
      "entropy: 0.4115437163836815\n",
      "entropy: 0.19689969062361276\n",
      "entropy: 0.3174709653533458\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 96\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35220228679551996\n",
      "entropy: 0.2869357022627195\n",
      "entropy: 0.3510807197099077\n",
      "entropy: 0.4115874217501056\n",
      "entropy: 0.19681387940583603\n",
      "entropy: 0.31745895233007765\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 97\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.35218920525150965\n",
      "entropy: 0.2867443457523856\n",
      "entropy: 0.3511069643341216\n",
      "entropy: 0.4116536411020195\n",
      "entropy: 0.19670116270239182\n",
      "entropy: 0.31743758460516425\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 98\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3521761420125828\n",
      "entropy: 0.2865599093127609\n",
      "entropy: 0.3511342477099622\n",
      "entropy: 0.41169906407722207\n",
      "entropy: 0.1966188190226146\n",
      "entropy: 0.31742725850914666\n",
      "-----------------------------------------------------------\n",
      "Starting training epoch: 99\n",
      "Batch size: 16 , Total number of training samples: 100\n",
      "-----------------------------------------------------------\n",
      "entropy: 0.3521634504267999\n",
      "entropy: 0.286384660773578\n",
      "entropy: 0.3511625624762384\n",
      "entropy: 0.41175284195790596\n",
      "entropy: 0.19652470403815667\n",
      "entropy: 0.31741238719908355\n",
      "Percentage Correct Classifications: 0.86\n",
      "Total Training Time: 472.16954278945923\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda.elementwise import ElementwiseKernel\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "MAX_ENTROPY = 1\n",
    "\n",
    "def cross_entropy(predictions=None, ground_truth=None):\n",
    "    \n",
    "    if predictions is None or ground_truth is None:\n",
    "        raise Exception(\"Error!  Both predictions and ground truth must be float32 arrays\")\n",
    "    \n",
    "    p = np.array(predictions).copy()\n",
    "    y = np.array(ground_truth).copy()\n",
    "    \n",
    "    if p.shape != y.shape:\n",
    "        raise Exception(\"Error!  Both predictions and ground_truth must have same shape.\")\n",
    "    \n",
    "    if len(p.shape) != 2:\n",
    "        raise Exception(\"Error!  Both predictions and ground_truth must be 2D arrays.\")\n",
    "    \n",
    "    total_entropy = 0\n",
    "    \n",
    "    for i in range(p.shape[0]):\n",
    "        for j in range(p.shape[1]):\n",
    "            \n",
    "            if y[i,j] == 1:            \n",
    "                total_entropy += min( np.abs( np.nan_to_num(  np.log( p[i,j] ) ) ) , MAX_ENTROPY)             \n",
    "            else:             \n",
    "                total_entropy += min( np.abs( np.nan_to_num( np.log( 1 - p[i,j] ) ) ), MAX_ENTROPY)\n",
    "    \n",
    "    return total_entropy / p.size\n",
    "\n",
    "\n",
    "DenseEvalCode = '''\n",
    "#define _RELU(x) ( ((x) > 0.0f) ? (x) : 0.0f )\n",
    "#define _SIGMOID(x)  ( 1.0f / (1.0f + expf(-(x)) ))\n",
    "__global__ void dense_eval(int num_outputs, int num_inputs, int relu, int sigmoid, float * w, float * b, \\\n",
    "                           float * x, float *y, int batch_size, int w_t, int b_t, float delta)\n",
    "{\n",
    "     int i = blockDim.x*blockIdx.x + threadIdx.x;\n",
    "     \n",
    "        \n",
    "     if (i < num_outputs)\n",
    "     {\n",
    "         for(int k=0; k < batch_size; k++)\n",
    "         {    \n",
    "              double temp = 0.0f;\n",
    "              \n",
    "              for (int j = 0; j < num_inputs; j++)\n",
    "              {\n",
    "                  temp += ((double) w[ (num_inputs) * i + j ] ) * ( (double) x[k * num_inputs + j]);\n",
    "              }\n",
    "                  \n",
    "              temp += (double) b[i];\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              y[k * num_outputs + i] = (float) temp;                 \n",
    "         }\n",
    "    \n",
    "         \n",
    "        \n",
    "        if( w_t >= 0 && i == (w_t / num_inputs))\n",
    "        {\n",
    "              int j = w_t % num_inputs;\n",
    "              \n",
    "              for(int k=0; k < batch_size; k++)\n",
    "                  y[k*num_outputs + i] += delta*x[k*num_inputs+j];\n",
    "                  \n",
    "              \n",
    "        }\n",
    "         \n",
    "        if( b_t >= 0 && i == b_t )\n",
    "        {\n",
    "              //int j = b_t % num_inputs;\n",
    "              \n",
    "              for(int k=0; k < batch_size; k++)\n",
    "                  y[k*num_outputs + i] += delta;\n",
    "        }\n",
    "    \n",
    "    \n",
    "        if(relu > 0 || sigmoid > 0)\n",
    "             for(int k=0; k < batch_size; k++)\n",
    "             {    \n",
    "                  float temp = y[k * num_outputs + i];\n",
    "                  \n",
    "                  if (relu > 0)\n",
    "                      temp = _RELU(temp);\n",
    "                      \n",
    "                  if (sigmoid > 0)\n",
    "                      temp = _SIGMOID(temp);\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  y[k * num_outputs + i] = temp;                 \n",
    "             }\n",
    "            \n",
    "    \n",
    "    }\n",
    "    \n",
    "    \n",
    "         \n",
    "    return;\n",
    "}\n",
    "'''\n",
    "\n",
    "eval_mod = SourceModule(DenseEvalCode)\n",
    "eval_ker = eval_mod.get_function('dense_eval')\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, num_inputs=None, num_outputs=None, weights=None, b=None, stream=None, \\\n",
    "    relu=False, sigmoid=False, delta=None):\n",
    "        \n",
    "        self.stream = stream\n",
    "        \n",
    "        if delta is None:\n",
    "            self.delta = np.float32(0.001)\n",
    "        else:\n",
    "            self.delta = np.float32(delta)\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = (np.random.rand(num_outputs, num_inputs) -.5 ) \n",
    "            self.num_inputs = np.int32(num_inputs)\n",
    "            self.num_outputs = np.int32(num_outputs)            \n",
    "        \n",
    "        if type(weights) != pycuda.gpuarray.GPUArray:\n",
    "            self.weights = gpuarray.to_gpu_async(np.array(weights, dtype=np.float32) , stream = self.stream)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        \n",
    "        if num_inputs is None or num_outputs is None:\n",
    "            \n",
    "            self.num_inputs = np.int32(self.weights.shape[1])\n",
    "            self.num_outputs = np.int32(self.weights.shape[0])\n",
    "            \n",
    "        else:\n",
    "            self.num_inputs = np.int32(num_inputs)\n",
    "            self.num_outputs = np.int32(num_outputs)\n",
    "\n",
    "\n",
    "        if b is None:\n",
    "            b = gpuarray.zeros((self.num_outputs,),dtype=np.float32)\n",
    "            \n",
    "        if type(b) != pycuda.gpuarray.GPUArray:\n",
    "            self.b = gpuarray.to_gpu_async(np.array(b, dtype=np.float32) , stream = self.stream)\n",
    "        else:\n",
    "            self.b = b   \n",
    "        \n",
    "        self.relu = np.int32(relu)\n",
    "        self.sigmoid = np.int32(sigmoid)\n",
    "        \n",
    "        self.block = (32,1,1)\n",
    "        \n",
    "        self.grid = (int(np.ceil(self.num_outputs / 32)), 1,1)\n",
    "        \n",
    "\n",
    "    def eval_(self, x, y=None, batch_size=None, stream=None, delta=None, w_t = None, b_t = None):\n",
    "    \n",
    "        if stream is None:\n",
    "            stream = self.stream\n",
    "        \n",
    "        if type(x) != pycuda.gpuarray.GPUArray:\n",
    "            x = gpuarray.to_gpu_async(np.array(x,dtype=np.float32) , stream=self.stream)\n",
    "            \n",
    "        if batch_size is None:\n",
    "            if len(x.shape) == 2:\n",
    "                batch_size = np.int32(x.shape[0])\n",
    "            else:\n",
    "                batch_size = np.int32(1)\n",
    "                \n",
    "        if delta is None:\n",
    "            delta = self.delta\n",
    "            \n",
    "        delta = np.float32(delta)\n",
    "            \n",
    "        if w_t is None:\n",
    "            w_t = np.int32(-1)\n",
    "            \n",
    "        if b_t is None:\n",
    "            b_t = np.int32(-1)\n",
    "        \n",
    "        \n",
    "        if y is None:\n",
    "            if batch_size == 1:\n",
    "                y = gpuarray.empty((self.num_outputs,), dtype=np.float32)\n",
    "            else:\n",
    "                y = gpuarray.empty((batch_size, self.num_outputs), dtype=np.float32)\n",
    "\n",
    "\n",
    "        eval_ker(self.num_outputs, self.num_inputs, self.relu, self.sigmoid, \\\n",
    "                 self.weights, self.b, x, y, np.int32(batch_size), w_t, b_t, \\\n",
    "                 delta , block=self.block, grid=self.grid , stream=stream)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "# threads: at least \"num\"\n",
    "SoftmaxExpCode='''\n",
    "__global__ void softmax_exp( int num, float *x, float *y, int batch_size)\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (i < num)\n",
    "    {\n",
    "        for (int k=0; k < batch_size; k++)\n",
    "        {\n",
    "            y[num*k + i] = expf(x[num*k+i]);\n",
    "        \n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "exp_mod = SourceModule(SoftmaxExpCode)\n",
    "exp_ker = exp_mod.get_function('softmax_exp')\n",
    "\n",
    "# threads: at least batch size\n",
    "SoftmaxMeanCode='''\n",
    "__global__ void softmax_mean( int num, float *x, float *y, int batch_size)\n",
    "{\n",
    "    // parallelize over\n",
    "    int i = blockDim.x*blockIdx.x + threadIdx.x;\n",
    "    \n",
    "    if (i < batch_size)\n",
    "    {\n",
    "        float temp = 0.0f;\n",
    "        \n",
    "        for(int k=0; k < num; k++)\n",
    "            temp += x[i*num + k];\n",
    "            \n",
    "        \n",
    "        for(int k=0; k < num; k++)\n",
    "            y[i*num+k] = x[i*num+k] / temp;\n",
    "    \n",
    "    }\n",
    "    \n",
    "    return;\n",
    "}'''\n",
    "\n",
    "mean_mod = SourceModule(SoftmaxMeanCode)\n",
    "mean_ker = mean_mod.get_function('softmax_mean')\n",
    "\n",
    "        \n",
    "class SoftmaxLayer:\n",
    "    def __init__(self, num=None, stream=None):\n",
    "        self.num = np.int32(num)\n",
    "        self.stream = stream\n",
    "        \n",
    "        \n",
    "    def eval_(self, x, y=None, batch_size=None, stream=None):\n",
    "        \n",
    "        if stream is None:\n",
    "            stream = self.stream\n",
    "        \n",
    "        if type(x) != pycuda.gpuarray.GPUArray:\n",
    "            temp = np.array(x,dtype=np.float32)\n",
    "            x = gpuarray.to_gpu_async( temp , stream=stream)\n",
    "            \n",
    "        if batch_size==None:\n",
    "            if len(x.shape) == 2:\n",
    "                batch_size = np.int32(x.shape[0])\n",
    "            else:\n",
    "                batch_size = np.int32(1)\n",
    "        else:\n",
    "            batch_size = np.int32(batch_size)\n",
    "        \n",
    "        \n",
    "        if y is None:\n",
    "            if batch_size == 1:\n",
    "                y = gpuarray.empty((self.num,), dtype=np.float32)\n",
    "            else:\n",
    "                y = gpuarray.empty((batch_size, self.num), dtype=np.float32)\n",
    "\n",
    "                \n",
    "        exp_ker(self.num, x, y, batch_size, block=(32,1,1), grid=(int( np.ceil( self.num / 32) ), 1, 1), stream=stream)\n",
    "        \n",
    "        mean_ker(self.num, y, y, batch_size, block=(32,1,1), grid=(int( np.ceil( batch_size / 32)), 1,1), stream=stream)\n",
    "    \n",
    "        return y\n",
    "    \n",
    "\n",
    "class SequentialNetwork:\n",
    "\n",
    "    def __init__(self, layers=None, delta=None, stream = None, max_batch_size=32, max_streams=10, epochs = 10):\n",
    "        \n",
    "        self.network = []\n",
    "        self.network_summary = []\n",
    "        self.network_mem = []\n",
    "        \n",
    "        if stream is not None:\n",
    "            self.stream = stream\n",
    "        else:\n",
    "            self.stream = drv.Stream()\n",
    "            \n",
    "            \n",
    "        if delta is None:\n",
    "            delta = 0.0001\n",
    "            \n",
    "        self.delta = delta\n",
    "        self.max_batch_size=max_batch_size\n",
    "        \n",
    "        self.max_streams = max_streams\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        if layers is not None:\n",
    "            for layer in layers:\n",
    "                add_layer(self, layer)\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "    \n",
    "        if layer['type'] == 'dense':\n",
    "            if len(self.network) == 0:\n",
    "                num_inputs = layer['num_inputs']\n",
    "            else:\n",
    "                num_inputs = self.network_summary[-1][2]\n",
    "            \n",
    "            num_outputs = layer['num_outputs']\n",
    "            sigmoid = layer['sigmoid']\n",
    "            relu = layer['relu']\n",
    "            \n",
    "            weights = layer['weights']\n",
    "            \n",
    "            b = layer['bias']\n",
    "            \n",
    "            self.network.append(DenseLayer(num_inputs=num_inputs, num_outputs=num_outputs, sigmoid=sigmoid, relu=relu, weights=weights, b=b))\n",
    "            self.network_summary.append( ('dense', num_inputs, num_outputs))\n",
    "            \n",
    "            if self.max_batch_size > 1:\n",
    "                if len(self.network_mem) == 0:\n",
    "                    self.network_mem.append(gpuarray.empty( (self.max_batch_size, self.network_summary[-1][1] ), dtype=np.float32 ) )\n",
    "                self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32  ) ) \n",
    "            else:\n",
    "                if len(self.network_mem) == 0:\n",
    "                    self.network_mem.append( gpuarray.empty( (self.network_summary[-1][1], ), dtype=np.float32 ) )\n",
    "                self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32  ) ) \n",
    "    \n",
    "        elif layer['type'] == 'softmax':\n",
    "            \n",
    "            if len(self.network) == 0:\n",
    "                raise Exception(\"Error!  Softmax layer can't be first!\")\n",
    "            \n",
    "            if self.network_summary[-1][0] != 'dense':\n",
    "                raise Exception(\"Error!  Need a dense layer before a softmax layer!\")\n",
    "            \n",
    "            \n",
    "            num = self.network_summary[-1][2]\n",
    "            \n",
    "            self.network.append(SoftmaxLayer(num=num))\n",
    "            \n",
    "            self.network_summary.append(('softmax', num, num))\n",
    "            \n",
    "            if self.max_batch_size > 1:\n",
    "                self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32  ) ) \n",
    "            else:\n",
    "                self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32  ) ) \n",
    "\n",
    "    \n",
    "    def predict(self, x, stream=None):\n",
    "        \n",
    "        if stream is None:\n",
    "            stream = self.stream\n",
    "        \n",
    "        if type(x) != np.ndarray:\n",
    "            temp = np.array(x, dtype = np.float32)\n",
    "            x = temp\n",
    "        \n",
    "        if(x.size == self.network_mem[0].size):\n",
    "            self.network_mem[0].set_async(x, stream=stream)\n",
    "        else:\n",
    "            \n",
    "            if x.size > self.network_mem[0].size:\n",
    "                raise Exception(\"Error: batch size too large for input.\")\n",
    "            \n",
    "            x0 = np.zeros((self.network_mem[0].size,), dtype=np.float32)\n",
    "            x0[0:x.size] = x.ravel()\n",
    "            self.network_mem[0].set_async(x0.reshape(self.network_mem[0].shape), stream=stream)\n",
    "        \n",
    "        if(len(x.shape) == 2):\n",
    "            batch_size = x.shape[0]\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        \n",
    "        for i in range(len(self.network)):\n",
    "            self.network[i].eval_(x=self.network_mem[i], y = self.network_mem[i+1], batch_size=batch_size, stream = stream)\n",
    "            \n",
    "        y = self.network_mem[-1].get_async(stream=stream)\n",
    "        \n",
    "        if len(y.shape) == 2:\n",
    "            y = y[0:batch_size, :]\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def partial_predict(self, layer_index=None, w_t=None, b_t=None, partial_mem=None, stream=None, batch_size=None, delta=None):\n",
    "        \n",
    "        self.network[layer_index].eval_(x=self.network_mem[layer_index], y = partial_mem[layer_index+1], batch_size=batch_size, stream = stream, w_t=w_t, b_t=b_t, delta=delta)\n",
    "        \n",
    "        for i in range(layer_index+1, len(self.network)):\n",
    "            self.network[i].eval_(x=partial_mem[i], y =partial_mem[i+1], batch_size=batch_size, stream = stream)\n",
    "\n",
    "            \n",
    "    \n",
    "    def bsgd(self, training=None, labels=None, delta=None, max_streams = None, batch_size = None, epochs = 1, training_rate=0.01):\n",
    "        \n",
    "        training_rate = np.float32(training_rate)\n",
    "        \n",
    "        training = np.float32(training)\n",
    "        labels = np.float32(labels)\n",
    "        \n",
    "        if( training.shape[0] != labels.shape[0] ):\n",
    "            raise Exception(\"Number of training data points should be same as labels!\")\n",
    "        \n",
    "\n",
    "        if max_streams is None:\n",
    "            max_streams = self.max_streams\n",
    "            \n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "            \n",
    "        if delta is None:\n",
    "            delta = self.delta\n",
    "        \n",
    "        streams = []\n",
    "        bgd_mem = []\n",
    "        \n",
    "        # create the streams needed for training \n",
    "        for _ in range(max_streams):\n",
    "            streams.append(drv.Stream())\n",
    "            bgd_mem.append([])\n",
    "            \n",
    "        \n",
    "        # allocate memory for each stream\n",
    "        \n",
    "        for i in range(len(bgd_mem)):\n",
    "            for mem_bank in self.network_mem:\n",
    "                bgd_mem[i].append( gpuarray.empty_like(mem_bank) )\n",
    "        \n",
    "        \n",
    "        # begin training!\n",
    "        \n",
    "        num_points = training.shape[0]\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = self.max_batch_size\n",
    "        \n",
    "        index = range(training.shape[0])\n",
    "        \n",
    "        for k in range(epochs):    \n",
    "            \n",
    "            print('-----------------------------------------------------------')\n",
    "            print('Starting training epoch: %s' % k)\n",
    "            print('Batch size: %s , Total number of training samples: %s' % (batch_size, num_points))\n",
    "            print('-----------------------------------------------------------')\n",
    "            \n",
    "            all_grad = []\n",
    "            \n",
    "            np.random.shuffle(list(index))\n",
    "            \n",
    "            for r in range( int(np.floor(training.shape[0] / batch_size)) ):\n",
    "            \n",
    "                batch_index = index[r*batch_size:(r+1)*batch_size] \n",
    "                \n",
    "                batch_training = training[batch_index, :]\n",
    "                batch_labels = labels[batch_index, :]\n",
    "                \n",
    "                batch_predictions = self.predict(batch_training)\n",
    "        \n",
    "                cur_entropy = cross_entropy(predictions=batch_predictions, ground_truth=batch_labels)\n",
    "                \n",
    "                print('entropy: %s' % cur_entropy)\n",
    "                \n",
    "                # need to iterate over each weight / bias , check entropy\n",
    "                \n",
    "                for i in range(len(self.network)):\n",
    "                    \n",
    "                    if self.network_summary[i][0] != 'dense':\n",
    "                        continue\n",
    "                    \n",
    "                    all_weights = Queue()\n",
    "                    \n",
    "                    grad_w = np.zeros((self.network[i].weights.size,), dtype=np.float32)\n",
    "                    grad_b = np.zeros((self.network[i].b.size,), dtype=np.float32)\n",
    "                    \n",
    "                    for w in range( self.network[i].weights.size ):\n",
    "                        all_weights.put( ('w', np.int32(w) ) )\n",
    "                        \n",
    "                    for b in range( self.network[i].b.size ):\n",
    "                        all_weights.put(('b', np.int32(b) ) )\n",
    "                        \n",
    "                    while not all_weights.empty():\n",
    "                        \n",
    "                        stream_weights = Queue()\n",
    "                        \n",
    "                        for j in range(max_streams):\n",
    "                            \n",
    "                            if all_weights.empty():\n",
    "                                break\n",
    "                            \n",
    "                            wb = all_weights.get()\n",
    "                            \n",
    "                            if wb[0] == 'w':\n",
    "                                w_t = wb[1]\n",
    "                                b_t = None\n",
    "                            elif wb[0] == 'b':\n",
    "                                b_t = wb[1]\n",
    "                                w_t = None\n",
    "                            \n",
    "                            stream_weights.put( wb )\n",
    "                            \n",
    "                            self.partial_predict(layer_index=i, w_t=w_t, b_t=b_t, partial_mem=bgd_mem[j], stream=streams[j], batch_size=batch_size, delta=delta)\n",
    "                            \n",
    "                        for j in range(max_streams):\n",
    "                            \n",
    "                            if stream_weights.empty():\n",
    "                                break\n",
    "                            \n",
    "                            wb = stream_weights.get()\n",
    "                            \n",
    "                            w_predictions = bgd_mem[j][-1].get_async(stream=streams[j])\n",
    "                            \n",
    "                            w_entropy = cross_entropy(predictions=w_predictions[:batch_size,:], ground_truth=batch_labels)\n",
    "                            \n",
    "\n",
    "                            if wb[0] == 'w':\n",
    "                                w_t = wb[1]\n",
    "                                grad_w[w_t] = -(w_entropy - cur_entropy) / delta\n",
    "                                \n",
    "                            elif wb[0] == 'b':\n",
    "                                b_t = wb[1]\n",
    "                                grad_b[b_t] = -(w_entropy - cur_entropy) / delta\n",
    "                        \n",
    "                    all_grad.append([np.reshape(grad_w,self.network[i].weights.shape) , grad_b])\n",
    "            \n",
    "            for i in range(len(self.network)):\n",
    "                \n",
    "                if self.network_summary[i][0] == 'dense':\n",
    "                \n",
    "                    new_weights = self.network[i].weights.get()\n",
    "                    new_weights += training_rate*all_grad[i][0]\n",
    "                    new_bias = self.network[i].b.get()\n",
    "                    new_bias += training_rate*all_grad[i][1]\n",
    "                    self.network[i].weights.set(new_weights)\n",
    "                    self.network[i].b.set(new_bias)\n",
    "                         \n",
    "            \n",
    "def condition_data(data, means=None, stds=None):\n",
    "    \n",
    "    if means is None:\n",
    "        means = np.mean(data, axis=0)\n",
    "        \n",
    "    if stds is None:\n",
    "        stds = np.std(data, axis = 0)\n",
    "        \n",
    "    conditioned_data = data.copy()\n",
    "    conditioned_data -= means\n",
    "    conditioned_data /= stds\n",
    "    \n",
    "    return (conditioned_data, means, stds)\n",
    "                        \n",
    "                            \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     to_class = { 'Iris-setosa' : [1,0,0] , 'Iris-versicolor' : [0,1,0], 'Iris-virginica' : [0,0,1]}\n",
    "     \n",
    "     iris_data = []\n",
    "     iris_labels = []\n",
    "     \n",
    "     with open('iris.data', 'rt') as csvfile:\n",
    "         csvreader = csv.reader(csvfile, delimiter=',')\n",
    "         for row in csvreader:\n",
    "             newrow = []\n",
    "             if len(row) != 5:\n",
    "                 break\n",
    "             for i in range(4):\n",
    "                 newrow.append(row[i])\n",
    "             iris_data.append(newrow)\n",
    "             iris_labels.append(to_class[row[4]])\n",
    "             \n",
    "     iris_len = len(iris_data)\n",
    "     shuffled_index = list(range(iris_len))\n",
    "     np.random.shuffle(shuffled_index)\n",
    "     \n",
    "     \n",
    "     iris_data = np.float32(iris_data)\n",
    "     iris_labels = np.float32(iris_labels)\n",
    "     iris_data = iris_data[shuffled_index, :]\n",
    "     iris_labels = iris_labels[shuffled_index,:]\n",
    "     \n",
    "     t_len = (2*iris_len) // 3\n",
    "     \n",
    "     iris_train = iris_data[:t_len, :]\n",
    "     label_train = iris_labels[:t_len, :]\n",
    "     \n",
    "     iris_test = iris_data[t_len:,:]\n",
    "     label_test = iris_labels[t_len:, :]\n",
    "     \n",
    "     \n",
    "     sn = SequentialNetwork( max_batch_size=32 )\n",
    "\n",
    "\n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 4, 'num_outputs' : 10, 'relu': True, 'sigmoid': False, 'weights' : None, 'bias' : None} )      \n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 10, 'num_outputs' : 15, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) \n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 15, 'num_outputs' : 20, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) \n",
    "     sn.add_layer({'type' : 'dense', 'num_inputs' : 20, 'num_outputs' : 3, 'relu': True, 'sigmoid': False, 'weights': None , 'bias': None } )   \n",
    "     sn.add_layer({'type' : 'softmax'})\n",
    "     \n",
    "     ctrain, means, stds = condition_data(iris_train)\n",
    "     \n",
    "\n",
    "     t1 = time()\n",
    "     sn.bsgd(training=ctrain, labels=label_train, batch_size=16, max_streams=10, epochs=100 , delta=0.0001,\n",
    "     training_rate=1)\n",
    "     training_time = time() - t1\n",
    "     \n",
    "     hits = 0\n",
    "     ctest, _, _ = condition_data(iris_test, means=means, stds=stds)\n",
    "     for i in range(ctest.shape[0]):\n",
    "         if np.argmax(sn.predict(ctest[i,:])) == np.argmax(label_test[i,:]):\n",
    "             hits += 1\n",
    "     \n",
    "     \n",
    "     print('Percentage Correct Classifications: %s' % (float(hits ) / ctest.shape[0]))\n",
    "     print('Total Training Time: %s' % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33982229682c888955ca524121202fe70bc1f8616e2a0962e61b4bee7840fbc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
