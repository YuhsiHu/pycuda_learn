{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演示CUDA流的用法及其所带来的性能提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 2.445618\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# 指定需要处理多少个数组\n",
    "num_arrays = 200\n",
    "# 指定数组大小\n",
    "array_len = 1024**2\n",
    "\n",
    "# 遍历数组中的所有元素，先乘2，再除以2，并进行50次这样的运算\n",
    "ker = SourceModule(\"\"\"       \n",
    "__global__ void mult_ker(float * array, int array_len)\n",
    "{\n",
    "     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     int num_iters = array_len / blockDim.x;\n",
    "     for(int j=0; j < num_iters; j++)\n",
    "     {\n",
    "         int i = j * blockDim.x + thd;\n",
    "         for(int k = 0; k < 50; k++)\n",
    "         {\n",
    "              array[i] *= 2.0;\n",
    "              array[i] /= 2.0;\n",
    "         }\n",
    "     }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "mult_ker = ker.get_function('mult_ker')\n",
    "\n",
    "data = []\n",
    "data_gpu = []\n",
    "gpu_out = []\n",
    "\n",
    "# generate random arrays.\n",
    "for _ in range(num_arrays):\n",
    "    data.append(np.random.randn(array_len).astype('float32'))\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "# copy arrays to GPU.\n",
    "for k in range(num_arrays):\n",
    "    data_gpu.append(gpuarray.to_gpu(data[k]))\n",
    "\n",
    "# process arrays.\n",
    "for k in range(num_arrays):\n",
    "    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1))\n",
    "\n",
    "# copy arrays from GPU.\n",
    "for k in range(num_arrays):\n",
    "    gpu_out.append(data_gpu[k].get())\n",
    "\n",
    "t_end = time()\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    assert (np.allclose(gpu_out[k], data[k]))\n",
    "\n",
    "print('Total time: %f' % (t_end - t_start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们对上面的程序稍加修改，使其使用CUDA流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.788665\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "num_arrays = 200\n",
    "array_len = 1024**2\n",
    "\n",
    "ker = SourceModule(\"\"\"       \n",
    "__global__ void mult_ker(float * array, int array_len)\n",
    "{\n",
    "     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     int num_iters = array_len / blockDim.x;\n",
    "     for(int j=0; j < num_iters; j++)\n",
    "     {\n",
    "         int i = j * blockDim.x + thd;\n",
    "         for(int k = 0; k < 50; k++)\n",
    "         {\n",
    "              array[i] *= 2.0;\n",
    "              array[i] /= 2.0;\n",
    "         }\n",
    "     }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "mult_ker = ker.get_function('mult_ker')\n",
    "\n",
    "data = []\n",
    "data_gpu = []\n",
    "gpu_out = []\n",
    "# 为每个单独的数组启动创建一个单独的流对象\n",
    "streams = []\n",
    "# 用新建的CUDA流对象来填充列表\n",
    "for _ in range(num_arrays):\n",
    "    streams.append(drv.Stream())\n",
    "\n",
    "# generate random arrays.\n",
    "for _ in range(num_arrays):\n",
    "    data.append(np.random.randn(array_len).astype('float32'))\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "# copy arrays to GPU.\n",
    "for k in range(num_arrays):\n",
    "    data_gpu.append(gpuarray.to_gpu_async(data[k], stream=streams[k]))\n",
    "\n",
    "# process arrays.\n",
    "for k in range(num_arrays):\n",
    "    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1), stream=streams[k])\n",
    "\n",
    "# copy arrays from GPU.\n",
    "for k in range(num_arrays):\n",
    "    gpu_out.append(data_gpu[k].get_async(stream=streams[k]))\n",
    "\n",
    "t_end = time()\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    assert (np.allclose(gpu_out[k], data[k]))\n",
    "\n",
    "print('Total time: %f' % (t_end - t_start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个CUDA流必须拥有自己的专用事件对象集合，多个CUDA流不能共享同一个事件对象。我们修改前面的程序，应用一下事件对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.798797\n",
      "Mean kernel duration (milliseconds): 13.582968\n",
      "Mean kernel standard deviation (milliseconds): 3.784286\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "num_arrays = 200\n",
    "array_len = 1024**2\n",
    "\n",
    "ker = SourceModule(\"\"\"       \n",
    "__global__ void mult_ker(float * array, int array_len)\n",
    "{\n",
    "     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     int num_iters = array_len / blockDim.x;\n",
    "     for(int j=0; j < num_iters; j++)\n",
    "     {\n",
    "         int i = j * blockDim.x + thd;\n",
    "         for(int k = 0; k < 50; k++)\n",
    "         {\n",
    "              array[i] *= 2.0;\n",
    "              array[i] /= 2.0;\n",
    "         }\n",
    "     }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "mult_ker = ker.get_function('mult_ker')\n",
    "\n",
    "data = []\n",
    "data_gpu = []\n",
    "gpu_out = []\n",
    "streams = []\n",
    "# 用事件对象来填充这些列表\n",
    "start_events = []\n",
    "end_events = []\n",
    "\n",
    "for _ in range(num_arrays):\n",
    "    streams.append(drv.Stream())\n",
    "    start_events.append(drv.Event())\n",
    "    end_events.append(drv.Event())\n",
    "\n",
    "# generate random arrays.\n",
    "for _ in range(num_arrays):\n",
    "    data.append(np.random.randn(array_len).astype('float32'))\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "# copy arrays to GPU.\n",
    "for k in range(num_arrays):\n",
    "    data_gpu.append(gpuarray.to_gpu_async(data[k], stream=streams[k]))\n",
    "\n",
    "# process arrays.\n",
    "for k in range(num_arrays):\n",
    "    # 记录开始\n",
    "    start_events[k].record(streams[k])\n",
    "    mult_ker(data_gpu[k], np.int32(array_len), block=(64,1,1), grid=(1,1,1), stream=streams[k])\n",
    "for k in range(num_arrays):\n",
    "    # 记录结束\n",
    "    end_events[k].record(streams[k])\n",
    "    \n",
    "# copy arrays from GPU.\n",
    "for k in range(num_arrays):\n",
    "    gpu_out.append(data_gpu[k].get_async(stream=streams[k]))\n",
    "\n",
    "t_end = time()\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    assert (np.allclose(gpu_out[k], data[k]))\n",
    "\n",
    "kernel_times = []\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    kernel_times.append(start_events[k].time_till(end_events[k]))\n",
    "\n",
    "print('Total time: %f' % (t_end - t_start))\n",
    "print('Mean kernel duration (milliseconds): %f' % np.mean(kernel_times))\n",
    "print('Mean kernel standard deviation (milliseconds): %f' % np.std(kernel_times))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续修改前面的代码，在生成的各个线程中运行用于完成乘、除运算的内核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import pycuda.driver as drv\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "from time import time\n",
    "import threading \n",
    "\n",
    "# 通常来说主机上创建的线程数量不宜超过20个\n",
    "num_arrays = 10\n",
    "array_len = 1024**2\n",
    "# 我们将原来的内核函数存储为字符串对象，因此它只能在一个上下文中进行编译，所以我们必须在各个线程中单独对其进行编译\n",
    "kernel_code = \"\"\"       \n",
    "__global__ void mult_ker(float * array, int array_len)\n",
    "{\n",
    "     int thd = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     int num_iters = array_len / blockDim.x;\n",
    "     for(int j=0; j < num_iters; j++)\n",
    "     {\n",
    "         int i = j * blockDim.x + thd;\n",
    "         for(int k = 0; k < 50; k++)\n",
    "         {\n",
    "              array[i] *= 2.0;\n",
    "              array[i] /= 2.0;\n",
    "         }\n",
    "     }\n",
    " \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "class KernelLauncherThread(threading.Thread):\n",
    "    def __init__(self, input_array):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.input_array = input_array\n",
    "        self.output_array = None\n",
    "  \n",
    "    def run(self):\n",
    "        # 选择设备\n",
    "        self.dev = drv.Device(0)\n",
    "        # 创建上下文\n",
    "        self.context = self.dev.make_context()\n",
    "        # 编译内核\n",
    "        self.ker = SourceModule(kernel_code)\n",
    "        # 提取内核函数的引用\n",
    "        self.mult_ker = self.ker.get_function('mult_ker')\n",
    "        # 将数组复制到GPU\n",
    "        self.array_gpu = gpuarray.to_gpu(self.input_array)\n",
    "        # 启动内核函数\n",
    "        self.mult_ker(self.array_gpu, np.int32(array_len), block=(64,1,1), grid=(1,1,1))\n",
    "        # 获取结果复制到主机\n",
    "        self.output_array = self.array_gpu.get()\n",
    "        # 销毁上下文\n",
    "        self.context.pop()\n",
    "        \n",
    "    def join(self):\n",
    "        # 将output_array返回至主机\n",
    "        threading.Thread.join(self)\n",
    "        return self.output_array\n",
    "\n",
    "drv.init()\n",
    "\n",
    "data = []\n",
    "gpu_out = []\n",
    "threads = []\n",
    "\n",
    "# generate random arrays and thread objects.\n",
    "for _ in range(num_arrays):\n",
    "    data.append(np.random.randn(array_len).astype('float32'))\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    # create a thread that uses data we just generated\n",
    "    threads.append(KernelLauncherThread(data[k]))\n",
    "\n",
    "# launch threads to process arrays.\n",
    "for k in range(num_arrays):\n",
    "    threads[k].start()\n",
    "    \n",
    "# get data from launched threads.\n",
    "for k in range(num_arrays):\n",
    "    gpu_out.append(threads[k].join())\n",
    "\n",
    "for k in range(num_arrays):\n",
    "    assert (np.allclose(gpu_out[k], data[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33982229682c888955ca524121202fe70bc1f8616e2a0962e61b4bee7840fbc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
