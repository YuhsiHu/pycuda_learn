{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 并行前缀算法的高效实现\n",
    "分为上行扫描阶段和下行扫描阶段，上行扫描阶段类似于一次规约操作，下行扫描对这些部分和进行处理给出最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does our work-efficient prefix work? True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy as np\n",
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "from time import time\n",
    "\n",
    "# kernel for up-sweep phase\n",
    "up_ker = SourceModule(\"\"\"\n",
    "__global__ void up_ker(double *x, double *x_old, int k )\n",
    "{\n",
    "     int tid =  blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     \n",
    "     int _2k = 1 << k;\n",
    "     int _2k1 = 1 << (k+1);\n",
    "     \n",
    "     int j = tid* _2k1;\n",
    "     \n",
    "     x[j + _2k1 - 1] = x_old[j + _2k -1 ] + x_old[j + _2k1 - 1];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "up_gpu = up_ker.get_function(\"up_ker\")\n",
    "\n",
    "# implementation of up-sweep phase\n",
    "def up_sweep(x):\n",
    "    # let's typecast to be safe.\n",
    "    x = np.float64(x)\n",
    "    x_gpu = gpuarray.to_gpu(np.float64(x) )\n",
    "    x_old_gpu = x_gpu.copy()\n",
    "    for k in range( int(np.log2(x.size) ) ) : \n",
    "        num_threads = int(np.ceil( x.size / 2**(k+1)))\n",
    "        grid_size = int(np.ceil(num_threads / 32))\n",
    "        \n",
    "        if grid_size > 1:\n",
    "            block_size = 32\n",
    "        else:\n",
    "            block_size = num_threads\n",
    "            \n",
    "        up_gpu(x_gpu, x_old_gpu, np.int32(k)  , block=(block_size,1,1), grid=(grid_size,1,1))\n",
    "        x_old_gpu[:] = x_gpu[:]\n",
    "        \n",
    "    x_out = x_gpu.get()\n",
    "    return(x_out)\n",
    "\n",
    "# kernel for down-sweep phase\n",
    "down_ker = SourceModule(\"\"\"\n",
    "__global__ void down_ker(double *y, double *y_old,  int k)\n",
    "{\n",
    "     int tid =  blockIdx.x*blockDim.x + threadIdx.x;\n",
    "     \n",
    "     int _2k = 1 << k;\n",
    "     int _2k1 = 1 << (k+1);\n",
    "     \n",
    "     int j = tid*_2k1;\n",
    "     \n",
    "     y[j + _2k - 1 ] = y_old[j + _2k1 - 1];\n",
    "     y[j + _2k1 - 1] = y_old[j + _2k1 - 1] + y_old[j + _2k - 1];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "down_gpu = down_ker.get_function(\"down_ker\")\n",
    "    \n",
    "# implementation of down-sweep phase\n",
    "def down_sweep(y):\n",
    "    y = np.float64(y)\n",
    "    y[-1] = 0\n",
    "    y_gpu = gpuarray.to_gpu(y)\n",
    "    y_old_gpu = y_gpu.copy()\n",
    "    for k in reversed(range(int(np.log2(y.size)))):\n",
    "        num_threads = int(np.ceil( y.size / 2**(k+1)))\n",
    "        grid_size = int(np.ceil(num_threads / 32))\n",
    "        \n",
    "        if grid_size > 1:\n",
    "            block_size = 32\n",
    "        else:\n",
    "            block_size = num_threads\n",
    "            \n",
    "        down_gpu(y_gpu, y_old_gpu, np.int32(k), block=(block_size,1,1), grid=(grid_size,1,1))\n",
    "        y_old_gpu[:] = y_gpu[:]\n",
    "    y_out = y_gpu.get()\n",
    "    return(y_out)\n",
    "     \n",
    "# full implementation of work-efficient parallel prefix sum\n",
    "def efficient_prefix(x):\n",
    "        return(down_sweep(up_sweep(x)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    testvec = np.random.randn(32*1024).astype(np.float64)\n",
    "    testvec_gpu = gpuarray.to_gpu(testvec)\n",
    "    \n",
    "    outvec_gpu = gpuarray.empty_like(testvec_gpu)\n",
    "     \n",
    "    prefix_sum = np.roll(np.cumsum(testvec), 1)\n",
    "    prefix_sum[0] = 0\n",
    "    \n",
    "    prefix_sum_gpu = efficient_prefix(testvec)\n",
    "    \n",
    "    print(\"Does our work-efficient prefix work? {}\".format(np.allclose(prefix_sum_gpu, prefix_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33982229682c888955ca524121202fe70bc1f8616e2a0962e61b4bee7840fbc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
